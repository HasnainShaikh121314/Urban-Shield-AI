{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af7244c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hasnain\\anaconda3\\envs\\rag_env_2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from openrouter import OpenRouter\n",
    "\n",
    "client = OpenRouter(api_key=\"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\")  # your key here\n",
    "\n",
    "# Load .env\n",
    "load_dotenv()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f352d93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(path):\n",
    "    doc = fitz.open(path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\") + \"\\n\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "590fafaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "\n",
    "translator = GoogleTranslator(source='ur', target='en')\n",
    "\n",
    "def is_urdu(text):\n",
    "    # Detect if the text contains Urdu characters (Unicode 0600–06FF)\n",
    "    for char in text:\n",
    "        if \"\\u0600\" <= char <= \"\\u06FF\":\n",
    "            return True\n",
    "    return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94112f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=500, overlap=100):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbfbe4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: earthquake_guidlines.pdf\n",
      "Processing: flood_guidlines_e.pdf\n",
      "Processing: forest_fire.pdf\n",
      "Processing: guidlines.pdf\n",
      "Translated Urdu to English.\n",
      "Processing: heatwave.pdf\n",
      "Processing: winter travel.pdf\n",
      "Processing: winter_guidlines.pdf\n",
      "Translated Urdu to English.\n",
      "Total chunks: 361\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "pdf_folder = \"documents/\"\n",
    "all_chunks = []\n",
    "\n",
    "# Function to translate Urdu to English safely in chunks\n",
    "import time\n",
    "from deep_translator import GoogleTranslator\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "\n",
    "translator = GoogleTranslator(source='ur', target='en')\n",
    "\n",
    "def translate_to_english(text, chunk_size=2000, retries=3, delay=2):\n",
    "    reshaped_text = get_display(arabic_reshaper.reshape(text))\n",
    "    translated_text = \"\"\n",
    "\n",
    "    for i in range(0, len(reshaped_text), chunk_size):\n",
    "        chunk = reshaped_text[i:i+chunk_size]\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                translated_chunk = translator.translate(chunk)\n",
    "                translated_text += translated_chunk + \" \"\n",
    "                break  # success, exit retry loop\n",
    "            except Exception as e:\n",
    "                print(f\"Translation error (attempt {attempt+1}):\", e)\n",
    "                time.sleep(delay)  # wait before retry\n",
    "                if attempt == retries - 1:\n",
    "                    translated_text += chunk  # fallback\n",
    "    return translated_text\n",
    "\n",
    "# Loop through PDFs\n",
    "for pdf_file in os.listdir(pdf_folder):\n",
    "    if pdf_file.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "        print(\"Processing:\", pdf_file)\n",
    "        \n",
    "        text = load_pdf(pdf_path)  # Your PDF reading function\n",
    "        \n",
    "        # Detect Urdu and translate\n",
    "        if is_urdu(text):  # Your language detection function\n",
    "            text = translate_to_english(text)\n",
    "            print(\"Translated Urdu to English.\")\n",
    "        \n",
    "        # Split into chunks\n",
    "        chunks = chunk_text(text)  # Your text chunking function\n",
    "        all_chunks.extend(chunks)\n",
    "\n",
    "print(\"Total chunks:\", len(all_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2f23b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FAISS index created and saved.\n"
     ]
    }
   ],
   "source": [
    "# English embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create embeddings\n",
    "embeddings = model.encode(all_chunks, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "# FAISS index\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(embeddings)\n",
    "\n",
    "# Save for later use\n",
    "os.makedirs(\"index\", exist_ok=True)\n",
    "faiss.write_index(index, \"index/faiss_index.bin\")\n",
    "np.save(\"index/chunks.npy\", np.array(all_chunks))\n",
    "\n",
    "print(\"✅ FAISS index created and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4aee008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, k=3):\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    distances, indices = index.search(np.array(query_embedding), k)\n",
    "    return [all_chunks[i] for i in indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86751d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query, retrieved_chunks):\n",
    "    context = \"\\n\\n\".join(retrieved_chunks)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant.\n",
    "Answer the question only using the provided context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "    response = client.chat.send(\n",
    "        model=\"openai/gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35165130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline(query, k=3):\n",
    "    retrieved = retrieve(query, k)\n",
    "    answer = generate_answer(query, retrieved)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7fe38374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assess damage to property and prioritize safety during cleanup efforts, seek medical attention for injuries or illnesses related to the flood, implement long-term flood mitigation measures like improving drainage systems and resilience, organize community cleanup efforts to remove debris and restore infrastructure, and coordinate with neighbouring communities and authorities for flood condition, mutual aid, and support.\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the flood safety measures?\"\n",
    "answer = rag_pipeline(question)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
